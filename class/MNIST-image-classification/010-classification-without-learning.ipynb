{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447416f6-2ca8-4ac6-bf1b-121c6a403cc2",
   "metadata": {},
   "source": [
    "# MNIST Part 1: Establishing a Baseline\n",
    "\n",
    "Before we train a neural net to recognize handwritten images, we'll try a simpler technique, just to see how it compares. I took most of this code and the explanations from chapter 4 of [FastAI book](https://colab.research.google.com/github/fastai/fastbook), but I tried to simplify things. When you're ready (and when you're interested), check out the fuller treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94d83f-7042-4e1f-8b4e-d7e1db0637c9",
   "metadata": {},
   "source": [
    "## Imports, setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa1302-3eb3-4246-87a6-e32348253f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "from fastai.vision.all import *\n",
    "fastbook.setup_book()\n",
    "\n",
    "matplotlib.rc('image', cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbbd88-3c90-4c78-acce-b56927ca5c03",
   "metadata": {},
   "source": [
    "Import subset of MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd79996-74d0-4918-82b0-88b49ebeff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9174b5-f12e-4b2c-b75e-3595b2129a6b",
   "metadata": {},
   "source": [
    "Set `BASE_PATH` to the location of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdcbcc-4596-4de5-96ea-023a12122763",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4687e0a-4ab9-492a-970f-3148a2ddc8d6",
   "metadata": {},
   "source": [
    "## What's in the the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fed226-5531-4472-a14e-f29ff936f640",
   "metadata": {},
   "source": [
    "We'll explore the data set a little. First, we can see a fairly typical structure for a ML project: \n",
    "  - a CSV file with labels\n",
    "  - a folder with training data\n",
    "  - a folder with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd62ef-2f85-4646-b046-89118443a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d3307-4d28-4c7c-b714-ae5562819e92",
   "metadata": {},
   "source": [
    "The training data is has a folder of images with handwritten 3s and another folder with images with handwritten 7s. (It'll be easier to start with just two digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010c127-08ae-4ea9-b034-bea2402f2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48d2a7-7cff-401c-93c0-9d309a8de38a",
   "metadata": {},
   "source": [
    "We'll created a list of all the images in each. I'm printing the `threes` so you can see what it looks like. (`sorted` just guarantees that we see the same list.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc85fd-84ed-44c2-9f7c-54def60aa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88da95f-3f78-49a2-a673-06ce4affa525",
   "metadata": {},
   "source": [
    "Here's what one of the images looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba20129-8963-4853-b23d-a290475fff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_three = Image.open(threes[1])\n",
    "show_image(ex_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca98b53-aacd-4313-8c90-6e7b9a8f451b",
   "metadata": {},
   "source": [
    "And here's a seven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8a9e8-9348-4c15-80d7-9420e884d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_seven = Image.open(sevens[3])\n",
    "show_image(ex_seven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89452aa-7fee-44af-8172-2c30826d1e78",
   "metadata": {},
   "source": [
    "## Arrays of gray-scale values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b55b1-dd03-4cfe-9608-ed8836507408",
   "metadata": {},
   "source": [
    "The Python notebook figures that we want to *see* these images *as images*, so that's how it renders them. Each is a square, 28 pixels x 28 pixels. Really, each image in the MNIST data set is a 2D array, a list of 28 lists, each with 28 values. Each of those values is a number between 0 and 255: 0 if it is white, 255 if it is black, and 254 shades of gray in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594049bf-043b-4b3c-90c8-129fbce46a9d",
   "metadata": {},
   "source": [
    "Here's what a part of that 2D array looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03892b-c95e-48d7-b7f1-c0de3873d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array(ex_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb4415-e1aa-4c4e-8a74-5339da003386",
   "metadata": {},
   "source": [
    "We can also represent it as a **tensor**, which is a fundamental data type in the `pytorch` machine library. We'll look more closely at tensors soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fa2bd-4e9a-4fe2-a128-c31237575bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(ex_three)[4:10,4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75522829-7d1a-471c-a8ba-4ab717f9b151",
   "metadata": {},
   "source": [
    "I'm going to take a slightly bigger slice -- just the top part of the digit, represented as a tensor -- and use the `pandas` library to show you individual pixels with their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee82342-17ea-443b-aa23-27d35f18c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_three_tensor = tensor(ex_three)\n",
    "df = pd.DataFrame(ex_three_tensor[4:15,4:22])\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209419e4-8399-4004-bb25-98ddd0607904",
   "metadata": {},
   "source": [
    "We could do the same for our example 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b9cd8-536f-4f1d-aa30-ab04b51383c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_seven_tensor = tensor(ex_seven)\n",
    "df = pd.DataFrame(ex_seven_tensor[4:15,4:22])\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeadab7-0cf7-4b93-93ea-c05bb7f193ef",
   "metadata": {},
   "source": [
    "## Baseline: Distinguish by Pixel Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc238ea-9802-4ff3-baa9-fbdecf7199f9",
   "metadata": {},
   "source": [
    "Before we try to train a model to distinguish 3s from 7s, let's think about how we could try to accomplish the same goal **without a model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37397c8-76aa-463b-913a-5820aa8f14e1",
   "metadata": {},
   "source": [
    "Maybe we could calculate an \"average 3\" and an \"average 7\" -- the \"image\" you'd get if you took the average pixel value across all the images. Then, when we want to make a prediction, we could see if the image is \"closer\" (whatever that means) to the average 3 or the average 4. \n",
    "\n",
    "(The \"average\", we hope, will let us skip the difficulties we'd face if we were trying to compare one specific digit to another specific digit, which may not be identically positioned or sized or curved, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273452e9-b280-4e74-92aa-f1bd94b41e14",
   "metadata": {},
   "source": [
    "We can use **tensors** to \"stack\" all the threes together. Try to picture all those 2D arrays stacked on top of each other so the top-left corner of each lines up with the top-left corner of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6bc42-781d-41b3-9cf9-afcd837cc45e",
   "metadata": {},
   "source": [
    "First, let's get a generate two lists where each element is a tensor representing one of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baaf3bb-68e1-431c-9625-a2bf8b71aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tensors = [tensor(Image.open(current_three)) for current_three in threes]\n",
    "seven_tensors = [tensor(Image.open(current_seven)) for current_seven in sevens]\n",
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b4eb8-be1a-4641-a3aa-32bccf0da331",
   "metadata": {},
   "source": [
    "Before we go any farther, let's take a moment to make sure the images we have in our tensor lists look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c020cee-837d-427d-b9dc-bcd7ffa31906",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(three_tensors[4321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7df54c-454b-4a3d-aa52-3ff20c9d5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(seven_tensors[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54245d43-29f4-4968-834c-f56b28495255",
   "metadata": {},
   "source": [
    "Looks good to me! Now let's stack them with the PyTorch `stack` method. While we're doing it, we'll also cast the integers to floating point numbers since that what PyTorch expects when we take an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef72b6-faf1-4783-aa71-5f9a0ca1423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(three_tensors).float()\n",
    "stacked_sevens = torch.stack(seven_tensors).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a2f07-1030-44e9-b38e-95b6f6c16b28",
   "metadata": {},
   "source": [
    "The resulting tensors (\"3-rank\" tensors, because they have three \"dimensions\" or \"ranks\") each have a `shape` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c1adb-a4ab-4ba0-a462-e0bfcca08b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cdb4a-f119-4d0a-8fd2-e4ceae3b58c7",
   "metadata": {},
   "source": [
    "That's 28 pixels x 28 pixels x 6131 images, all \"lined up\" to make calculations efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0f503-f990-40cd-8a52-d98f89455909",
   "metadata": {},
   "source": [
    "Now let's average all the three-images (or rather their tensor representations) along the *0-dimension* (the \"stacked\" dimension). The result will be the \"average\" or \"ideal\" three, a 28 x 28 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845d317-6385-4af7-b43b-2af6223c1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "average3 = stacked_threes.mean(0)\n",
    "show_image(average3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442ae00-968b-4b96-9d92-6e3044ebc491",
   "metadata": {},
   "source": [
    "It makes some intuitive sense that the average of all those images averaged together would produce a blurry three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eaeb69-4839-4912-86d6-8fd28f0f9c44",
   "metadata": {},
   "source": [
    "And the average or ideal 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b5475-20e3-429d-b320-140b9f1e4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "average7 = stacked_sevens.mean(0)\n",
    "show_image(average7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758f5b7-6a23-4bab-981b-2a72965ad9f1",
   "metadata": {},
   "source": [
    "The seven is kind of interesting because it's extra blurry at its tips. We could guess that those are the parts of the images where there's more variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a1d6a-f33c-4e2a-b10e-afa0961aea20",
   "metadata": {},
   "source": [
    "### Measuring the \"distance\" between an image and the averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1510e79-f0d4-415a-b007-1e38ebff92fd",
   "metadata": {},
   "source": [
    "Now that we have our average 3 and our average 7, let's take a random 3 image and see how similar it is to each of the average images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51c293-1eec-4e6b-8e97-bf1874448658",
   "metadata": {},
   "source": [
    "How should we measure similarity? You might think we could just sum the difference for each pixel pair. But some differences will be positive and some will be negative, and summing them will tend to cancel the differences. So what we want is a method that won't cancel the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adb33c-47a9-4374-a609-49b75d9e206e",
   "metadata": {},
   "source": [
    "#### Mean Absolute Difference\n",
    "\n",
    "One way to avoid self-cancelling differences is to take the average of the **absolute value** of the pixel-by-pixel differences. This method is calle **mean absolute difference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a372b3-c499-456a-94ea-e4372aefe4ff",
   "metadata": {},
   "source": [
    "We'll use our `ex_three_tensor` (defined above) as the specific image we want to compare to the averages. Here's how we'd calculate its similarity to the average 3 and the average 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942921d-1f41-461e-b162-e07665e3b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_3_abs = (ex_three_tensor - average3).abs().mean()\n",
    "dist_7_abs = (ex_three_tensor - average7).abs().mean()\n",
    "dist_3_abs, dist_7_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c4f02-61a3-47e4-bef4-d7fa5f98aa9b",
   "metadata": {},
   "source": [
    "The \"distance\" numbers themselves don't mean much. What's important is that the \"distance\" is smaller between our random 3 and our average 3 than the difference between our random 3 and our average 7. Using our method, we would predict that our random 3 is in fact a 3, or at least more likely to be a 3 than a 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941b3ff-20e8-4b6d-abfe-966f1bd2db0c",
   "metadata": {},
   "source": [
    "Let's try again, this time with a random 7. I'll also use a built in method from PyTorch. It looks different, but it's performing the **mean absolute difference** calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cede3-f548-4c72-ae44-153527ab302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_seven_tensor = tensor(ex_seven)\n",
    "dist_3_l1 = F.l1_loss(ex_seven_tensor.float(), average3)\n",
    "dist_7_l1 = F.l1_loss(ex_seven_tensor.float(), average7)\n",
    "dist_3_l1, dist_7_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d9aeb-647e-4b47-867b-2e9805e2dd89",
   "metadata": {},
   "source": [
    "Yatzee! Our random seven was closer to the average 7 than the average 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304de8c4-a118-4a2d-a617-ed90386939d3",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e289b-33b6-49e0-9ab6-728b0cd601f3",
   "metadata": {},
   "source": [
    "A second method to avoid self-cancelling differences is to take the average of the *squares* of the differences (because squares will always be positive) and then take the square root of that average (which undoes the squaring). Again, we'll implement twice, once \"long hand\" to test our random 3 and again, using a PyTorch method, to test our random 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9d69a-70ce-415a-9876-ace2652e69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_3_sqr = ((ex_three_tensor - average3)**2).mean().sqrt()\n",
    "dist_7_sqr = ((ex_three_tensor - average7)**2).mean().sqrt()\n",
    "dist_3_sqr, dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e471ed5-c1d6-4dab-9538-faeface3c2fc",
   "metadata": {},
   "source": [
    "Whew. It, too, shows that our random 3 is more like our average 3 than our average 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769664a-d5ee-4ce5-88cd-4e40f3926328",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_3_sqr = F.mse_loss(ex_seven_tensor, average3).sqrt()\n",
    "dist_7_sqr = F.mse_loss(ex_seven_tensor, average7).sqrt()\n",
    "dist_3_sqr, dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacb738-1b0d-400a-bcf1-8d7a11510af5",
   "metadata": {},
   "source": [
    "Nice. Our random 7 is more similar to our average 7 than our average 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0abfb-6086-4662-9f05-29eb8b6fa290",
   "metadata": {},
   "source": [
    "#### Mean Absolute Difference or Root Mean Squared Error??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b064615-0d93-41c0-a718-057a8b9da021",
   "metadata": {},
   "source": [
    "Both worked. So which should you prefer. The answer, as usual, is: it depends. But RMSE will tend to \"penalize\" big errors more and be (comparatively) more forgiving for small errors. That makes a lot of sense, and you're more likely to see RMSE \"in the wild\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d94b5-25b1-4ae1-b3a7-9d888e027994",
   "metadata": {},
   "source": [
    "### Is Our Similarity Comparison Good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bc34f-4e4e-494b-b36d-2c91b32bc8d5",
   "metadata": {},
   "source": [
    "We correctly (if somewhat tediously) classified two digits. But that shouldn't give us a ton of confidence. We'd like to know how well our method will generalize. If we have a couple thousand 3 and 7 images, how many can we correctly classify?\n",
    "\n",
    "We made an important but hard-to-spot shift, so let's slow down and repeat it. To judge our classifier, we don't want to measure the average *loss* or *distance* or *similarity* over our test set. Instead, we want to calculate a different metric, the ***error rate***, or the percentage of images correctly classified. *Loss* or *similarity* scores don't mean much. What matters is whether we make an accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c271fe3-9e1f-4488-bab3-ff3e9900872d",
   "metadata": {},
   "source": [
    "#### Preparing our validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3760291-c753-4f2d-a344-0d84e1ba5fcb",
   "metadata": {},
   "source": [
    "Because we haven't really \"trained\" a model, it's not so critical that we use a fresh set of images to validate our classifier. But the MNIST set already has a separate validation set, so we may as well use it.\n",
    "\n",
    "I'm going to \"stack\" up the all the validation images, but I'm not going to average them. This time, I'm creating that \"stacked\" tensor because we'll be able to run calculations on it more efficiently. This is also a chance to see how we can condense a lot of code we wrote above into just a few lines. Here goes . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50518c48-f516-48fd-a21e-eaf2c72eca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_3_tensors = torch.stack([tensor(Image.open(img_path)) for img_path in (path/'valid'/'3').ls()]).float()\n",
    "validation_7_tensors = torch.stack([tensor(Image.open(img_path)) for img_path in (path/'valid'/'7').ls()]).float()\n",
    "validation_3_tensors.shape, validation_7_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521392b-c783-449b-bd7b-af0b68b2ff38",
   "metadata": {},
   "source": [
    "You can see that we have a stack of 1010 3s and 1028 7s, each 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb4755-5bd5-4b8e-a97b-5a5abd1f7fde",
   "metadata": {},
   "source": [
    "Now we'd like to know the \"distance\" of each from *both* the average 3 and the average 7 so we can compare those distances and decide how to classify each: a 3 if it is more similar (less distant) to the average 3 than to the average 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e5dca-fcfa-464f-8e1d-dab572663342",
   "metadata": {},
   "source": [
    "Here's a little function to calculate the \"distance\" between two images (or, because of some optimzations that tensors make possible, between a stack of images and the average image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e698f4c-45e0-4b9d-ac90-b20555caa08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(a, b):\n",
    "    return (a-b).abs().mean((-1, -2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3826dd-8762-47d9-a196-324fdff1933f",
   "metadata": {},
   "source": [
    "We'll use the *mean absolute distance* method. `(-1, -2)` means we'll average the last and the second-to-last dimensions (so the second and third dimensions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d410711-d631-4f82-8d61-bcd23807a0eb",
   "metadata": {},
   "source": [
    "And here's a little function to determine if a given image is a 3 -- that is, if it is more similar to the average 3 than the average 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455ffd2-d43c-4a31-b434-a6b61ce0c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_3(x):\n",
    "    return calc_distance(x, average3) <= calc_distance(x, average7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55979a8c-3e49-470d-9362-1b05fe299c4d",
   "metadata": {},
   "source": [
    "(Notice that we'll give the tie to 3. In practice, I doubt we'll ever get identical distances, so nothing really rides on that decision.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ff9ea-ece1-4afb-aaf7-cd5cd2aae067",
   "metadata": {},
   "source": [
    "We can test out our little functions on our examples from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c0b6d-d904-44ab-8a7c-1fcd5aee63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_3(ex_three_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804286f-5a49-4116-b97d-ecb6363b6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_3(ex_seven_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62ea83-2d65-4285-b9e2-681af557d1aa",
   "metadata": {},
   "source": [
    "Okay, moment of truth. Let's see how many of our validation images `is_3` correctly classifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29c7d9-f5e2-4627-b837-f4fd98897bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_3s = is_3(validation_3_tensors).float().mean()\n",
    "accuracy_7s = (1 - is_3(validation_7_tensors).float()).mean()\n",
    "accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10f294-6ff5-4ea3-86ee-e0a81d0d059c",
   "metadata": {},
   "source": [
    "Not too bad! We correctly identified 7s more often than we correctly identified 3s, but between the two, we have ~95% accuracy. But before you get too excited, remember that 3 and 7 are not *that* similar and we made the task easier by only needing to distinguish between 2 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0a119-b9c5-41a2-b74d-234db4555274",
   "metadata": {},
   "source": [
    "(NOTE: you may have wondered how we could pass a whole \"stack\" of images to `is_3`. Didn't we write it to find the difference between just two images (one of which was the \"averaged\" image)? Well yes! But it works because PyTorch defines tensor operations so that the \"unstacked\" \"averaged\" image gets *broadcast* -- treated *as if* it had the same dimensions as the stack. On top of that, if executed on a GPU, it can do whole batches of those calculations in parallel.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
